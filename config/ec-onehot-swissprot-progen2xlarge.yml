save_folder: results
save_interval: 12000ba
run_name: ec-onehot-swissprot-progen2xlarge

model:
  pretrained_model: progen2-xlarge
  #below are all non-default parameters
  adapter_projection_nlayers: 2 #use MLP for projection
  adapter_nlayers: 4
  adapter_c_s: 256
data:
  shuffle: True
  prefetch_batch_size: 2000
  eval_tokens_per_batch: 12000 
  train_tokens_per_batch: 24000
  eval_interval: 6000ba
  seed: 9176
  ec_encoding: data/ec2level4_onehot.pt
  
  train_sources:
  - local: data/sharded_datasets/CARE_resampled50cluster_medium_withTax/train
  val_sources:
  - val70: 
    - local: data/sharded_datasets/CARE_resampled50cluster_medium_withTax/val70
  - val90: 
    - local: data/sharded_datasets/CARE_resampled50cluster_medium_withTax/val90
  - test: 
    - local: data/sharded_datasets/CARE_resampled50cluster_medium_withTax/test
train:
  seed: 42
  max_duration: 1.5e9 #this should be in units of tokens (epochs won't work)
  lr: 5e-4 
  weight_decay: 5e-06
  warmup_steps: 500 #batches
  total_lr_decay_factor: 0.1
  gradient_clipping_threshold: 0.8